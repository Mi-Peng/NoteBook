# Entropy and Cross Entropy and Kl divergence

### 熵 Entropy

我们用熵（Entropy）来描述一个系统或一个事件的不确定性。”熵“这一概念本身来自于热力学，香农的信息熵是本文的主要讨论内容。

> 回顾一下概率论，考虑一个随机试验 $E$ ，举例为”抛掷一颗骰子，观察出现的点数“；随机试验 $E$ 所有可能出现的基本事件（基本事件：结果单一不可再分解，每一个基本事件彼此不相容）组成的集合，称为样本空间 $S$ ，对于本例就是，$S=\{1点朝上,2点朝上,...\}$；考虑事件域 $\mathscr{F}$  中的一个元素/事件”骰子点数为偶数“，即$A=\{骰子点数为偶数\}$，用随机变量 $X$ 表示（随机变量），则 $X=\{2, 4, 6\}$；（随机变量是一个实值函数，将一个事件映射为一个值）

考虑一事件$A$，其对应离散随机变量$X$的取值范围为$\{x_1,x_2,...,x_N\}$,则离散随机变量$X$的信息熵定义如下：
$$
H(X)=-\sum_{i=1}^Np(x_i)\cdot \log p(x_i)
$$
其中$p(\cdot)$表示其中基本事件发生的概率。

无损编码长度的最小长度

### 交叉熵 Cross Entropy 

考虑两个概率分布$p,q$ 交叉熵定义如下：
$$
H(p,q)=-\sum_{x}p(x)\log q(x)
$$
表示用一个事件去编码另一个事件的冗余编码长度

### KL散度 Kullback-Leibler Divergence

