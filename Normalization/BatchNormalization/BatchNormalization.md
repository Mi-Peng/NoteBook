## Batch Normalization

### 1. What is Batch Normalization

&emsp;&emsp;ä¼ ç»Ÿæœºå™¨å­¦ä¹ /ç»Ÿè®¡åˆ†æä¸­ï¼Œä¸€èˆ¬è¦å¯¹è¾“å…¥çš„feature/dataåšscaleï¼Œå¸¸è§çš„æ–¹æ³•æœ‰ï¼š

* çº¿æ€§å½’ä¸€åŒ–(Min-Max Scaling)      
$$
\widehat{x}=\frac{x-x_{min}}{x_{max}-x_{min}}
$$

* 0å‡å€¼æ ‡å‡†åŒ–(Z-score standardization) 
$$
z=\frac{x-\mu}{\sigma}
$$

* ç›´æ–¹å›¾å‡è¡¡åŒ–(å›¾åƒå¤„ç†)

&emsp;&emsp;å—æ•°æ®é¢„å¤„ç†å¯å‘ï¼Œåœ¨æ¯ä¸€ä¸­é—´å±‚è¾“å…¥ä¹‹å‰éƒ½è¿›è¡Œé¢„å¤„ç†ï¼š

 <div align=center><img src="./figs/BN1.png" width = 60%/></div>

&emsp;&emsp;Batch Normalizationå±‚ç®—æ³•æ•´ä½“åˆ†æˆä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥è®¡ç®—ä¸€ä¸ªBatchä¸­çš„å‡å€¼ä¸æ–¹å·®å¯¹è¾“å…¥æ•°æ®åšæ ‡å‡†åŒ–ï¼Œç¬¬äºŒæ­¥å¯¹æ ‡å‡†åŒ–æ•°æ®åšscaleä¸shiftï¼Œå³ç¼©æ”¾ä¸å¹³ç§»ã€‚å…¶ä¸­çš„$\beta$ä¸$\gamma$æ˜¯é€šè¿‡å­¦ä¹ å¾—æ¥çš„ã€‚

&emsp;&emsp;Batch Normalizationåœ¨é¢„æµ‹é˜¶æ®µæ‰€æœ‰å‚æ•°éƒ½æ˜¯å›ºå®šå€¼ï¼Œ$\beta$å’Œ$\gamma$éšç€è®­ç»ƒç»“æŸï¼Œä¸¤è€…æœ€ç»ˆæ”¶æ•›ï¼Œé¢„æµ‹é˜¶æ®µä½¿ç”¨è®­ç»ƒç»“æŸæ—¶çš„å€¼ã€‚å¯¹äº$\mu$å’Œ$\sigma$ï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼Œå®ƒä»¬ä¸ºå½“å‰mini batchçš„ç»Ÿè®¡é‡ã€‚åœ¨é¢„æµ‹é˜¶æ®µåˆ™é‡‡ç”¨è®­ç»ƒæ”¶æ•›æœ€åå‡ æ‰¹mini batchçš„ $\mu$å’Œ$\sigma$çš„æœŸæœ›ï¼Œä½œä¸ºé¢„æµ‹é˜¶æ®µçš„$\mu$å’Œ$\sigma$ã€‚

<div align=center><img src="./figs/BN2.png" width="60%"></div>

&emsp;&emsp;å‡å¦‚æˆ‘è¦é¢„æµ‹ä¸€ä¸ªäººçš„å¥åº·çŠ¶å†µï¼Œæˆ‘ä»¬èº«é«˜ï¼Œä½“é‡ï¼Œå¹´é¾„ä¿¡æ¯ï¼Œbatchsizeä¸º10ï¼Œè¾“å…¥å¤§å°ä¸º[10, 3]ï¼Œæˆ‘ä»¬æ²¿ç€æ¯ä¸ªç‰¹å¾ç»´åº¦å»è®¡ç®—batché‡Œæ•°æ®çš„å‡å€¼å’Œæ–¹å·®ï¼Œå¾—åˆ°èº«é«˜çš„å‡å€¼æ–¹å·®ã€ä½“é‡çš„å‡å€¼æ–¹å·®ã€å¹´é¾„ä¿¡æ¯çš„å‡å€¼æ–¹å·®åšå½’ä¸€åŒ–ã€‚

| èº«é«˜ | ä½“é‡ | å¹´é¾„ |
|:----:|:----:|:----:|
| 150 | 54   | 26   |
| 60  | 18   | 4    |
| 160 | 60   | 48   |
| 177 | 53   | 16   |
| 180  | 60   | 24   |
| 192  | 90   | 22   |
| 172  | 76   | 38   |
| 168  | 83   | 47   |
| 172  | 64   | 17   |
| 180  | 90   | 25   |

&emsp;&emsp;ä»¥èº«é«˜ä¸ºä¾‹ï¼ŒBatch Normalizationæ“ä½œè®¡ç®—å‡å€¼ï¼š

$$
\mu_1 = \frac{1}{10}(150+60+160+177+180+192+172+168+172+180)=161.1
$$

### 2.Batch Normalization in Conv

&emsp;&emsp; å‡è®¾ä¸€ä¸ªå·ç§¯å±‚è¾“å…¥çš„sizeä¸º[b,c,h,w]ï¼Œå…¶ä¸­bä¸ºbatch sizeï¼Œcä¸ºchannelæ•°ï¼Œhä¸wä¸ºfeaturemapå¤§å°ã€‚Batch NormalizationæŒ‰ç…§é€šé“æ•°è®¡ç®—$\mu$ä¸$\sigma$å³ï¼š
$$
\mu_i = \frac{1}{b\times h\times w}\sum_{b,h,w}Input(b,i,h,w) \in \mathbb{R}^1 \\
\mu = [\mu_1,\mu_2,...,\mu_c] \in \mathbb{R}^c
$$

&emsp;&emsp; åŒç†$\sigma \in \mathbb{R}^c$ã€‚

&emsp;&emsp; å¯¹æ¯”ä¸Šä¸€ç« æœ€åçš„ä¾‹å­;å¯ä»¥çœ‹å‡ºï¼ŒConv2dçš„BNæ“ä½œå°†Channelè§†ä¸ºç‰¹å¾ï¼ŒChannelé€šé“å¯¹åº”çš„FeatureMapåœ¨Hï¼ŒWç»´åº¦å–å‡å€¼ä½œä¸ºè¯¥é€šé“çš„ç‰¹å¾å€¼ã€‚


###  3. Why use Batch Normalization

&emsp;&emsp;è€ƒè™‘ä¸€èˆ¬ç½‘ç»œç»“æ„ï¼š
<div align=center><img src='./figs/BN3.png' width=100%></div>

&emsp;&emsp;ä¸€æ¬¡åå‘ä¼ æ’­è¿‡ç¨‹ä¼šåŒæ—¶æ›´æ–°æ‰€æœ‰å±‚çš„æƒé‡ï¼Œå‰é¢å±‚æƒé‡çš„æ›´æ–°ä¼šæ”¹å˜å½“å‰å±‚è¾“å…¥çš„åˆ†å¸ƒï¼Œè€Œè·Ÿæ®åå‘ä¼ æ’­çš„è®¡ç®—æ–¹å¼ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œå¯¹å±‚æƒé‡çš„æ›´æ–°æ˜¯åœ¨å…¶è¾“å…¥ä¸å˜çš„æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚

&emsp;&emsp;æˆ‘ä»¬è€ƒè™‘æŸç½‘ç»œæŸå±‚ï¼Œå‡è®¾å…¶åªæœ‰ä¸¤ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå¯¹äºè¯¥å±‚çš„è¾“å‡ºèŠ‚ç‚¹ç›¸å½“äºä¸€ä¸ªçº¿æ€§æ¨¡å‹$y=w_1x_1+w_2x_2+b$ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<div align=center><img src='./figs/BN4.png' width=60%></div>


&emsp;&emsp;å‡å®šå½“å‰è¾“å…¥$x_1$å’Œ$x_2$çš„åˆ†å¸ƒå¦‚å›¾ä¸­åœ†ç‚¹æ‰€ç¤ºï¼Œæœ¬æ¬¡æ›´æ–°çš„æ–¹å‘æ˜¯å°†ç›´çº¿$H_1$æ›´æ–°æˆ$H_2$ï¼Œæœ¬ä»¥ä¸ºåˆ‡åˆ†å¾—ä¸é”™ï¼Œä½†æ˜¯å½“å‰é¢å±‚çš„æƒé‡æ›´æ–°å®Œæ¯•ï¼Œå½“å‰å±‚è¾“å…¥çš„åˆ†å¸ƒæ¢æˆäº†å¦å¤–ä¸€ç•ªæ ·å­ï¼Œç›´çº¿ç›¸å¯¹è¾“å…¥åˆ†å¸ƒçš„ä½ç½®å¯èƒ½å˜æˆäº†$H_3$ï¼Œä¸‹ä¸€æ¬¡æ›´æ–°åˆè¦æ ¹æ®æ–°çš„åˆ†å¸ƒé‡æ–°è°ƒæ•´ã€‚ç›´çº¿è°ƒæ•´äº†ä½ç½®ï¼Œè¾“å…¥åˆ†å¸ƒåˆåœ¨å‘ç”Ÿå˜åŒ–ï¼Œç›´çº¿å†è°ƒæ•´ä½ç½®ï¼Œå°±åƒæ˜¯ç›´çº¿å’Œåˆ†å¸ƒä¹‹é—´çš„â€œè¿½é€æ¸¸æˆâ€ã€‚

&emsp;&emsp;è¿™ç§æƒ…å†µå¯¹äºæµ…å±‚æ¨¡å‹æ¥è¯´å½±å“ä¸å¤§ï¼Œä½†å¯¹äºæ·±å±‚æ¨¡å‹ï¼Œæ¯å±‚çš„è¾“å…¥åˆ†å¸ƒä¸æƒé‡åŒæ—¶å˜åŒ–ï¼Œä½¿å¾—è®­ç»ƒç›¸å½“å›°éš¾ï¼Œä¸å®¹æ˜“æ”¶æ•›ã€‚ä»è€Œè¦ä½¿ç”¨å¾ˆå°çš„å­¦ä¹ ç‡æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

&emsp;&emsp;æŒ‰ç…§Batch NormalizationåŸæ–‡æè¿°ï¼Œæ¯å±‚æƒé‡çš„æ›´æ–°æ˜¯åœ¨å‡å®šå…¶ä»–æƒé‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå‘æŸå¤±å‡½æ•°é™ä½çš„æ–¹å‘è°ƒæ•´è‡ªå·±ã€‚é—®é¢˜åœ¨äºï¼Œåœ¨ä¸€æ¬¡åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰çš„æƒé‡ä¼šåŒæ—¶æ›´æ–°ï¼Œå¯¼è‡´å±‚é—´é…åˆâ€œç¼ºä¹é»˜å¥‘â€ï¼Œæ¯å±‚éƒ½åœ¨è¿›è¡Œä¸ŠèŠ‚æ‰€è¯´çš„â€œè¿½é€æ¸¸æˆâ€ï¼Œè€Œä¸”å±‚æ•°è¶Šå¤šï¼Œç›¸äº’é…åˆè¶Šå›°éš¾ï¼Œæ–‡ä¸­æŠŠè¿™ä¸ªç°è±¡ç§°ä¹‹ä¸º **Internal Covariate Shift**ã€‚ä¸ºäº†é¿å…è¿‡äºéœ‡è¡ï¼Œå­¦ä¹ ç‡ä¸å¾—ä¸è®¾ç½®å¾—è¶³å¤Ÿå°ï¼Œè¶³å¤Ÿå°å°±æ„å‘³ç€å­¦ä¹ ç¼“æ…¢ã€‚

&emsp;&emsp;é™¤æ­¤ä¹‹å¤–ï¼Œå¯¹äºæ¿€æ´»å‡½æ•°Sigmoidæ¥è¯´ï¼Œç”±äºå…¶é¥±å’Œæ€§ï¼Œå½“è¾“å…¥å€¼$x$ä¸åœ¨0é™„è¿‘æ—¶ï¼Œå…¶è¾“å‡ºçš„å¯¼æ•°å¾ˆå°ï¼Œç½‘ç»œå‚æ•°éš¾ä»¥å¾—åˆ°è®­ç»ƒï¼Œå¾ˆå®¹æ˜“å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±ï¼Œè€ŒBatch Normalizationå°†è¾“å…¥æ•°æ®æ ‡å‡†å½’ä¸€åŒ–ä¹‹åä½¿æ•°æ®åœ¨ä¿è¯æœ¬èº«ä¿¡æ¯çš„æƒ…å†µä¸‹å°½å¯èƒ½è½åœ¨çº¿æ€§åŒºã€‚
$$
Sigmoid: \ f(x)=\frac{1}{1+e^{-x}}
$$
 <div align=center><img src='./figs/sigmoid.png' width=70%></div>

### 4. Result

* åŠ é€Ÿè®­ç»ƒæ”¶æ•›
* æé«˜æ³›åŒ–èƒ½åŠ›(å˜ç›¸æ·»åŠ æ­£åˆ™åŒ–)
* é€‚åº”èŒƒå›´æ›´å¤§çš„å­¦ä¹ ç‡(w/oBNè¾ƒé«˜çš„å­¦ä¹ ç‡å‘æ•£ä¸æ”¶æ•›) 
* é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¢¯åº¦æ¶ˆå¤±
* ä¾èµ–Batch Sizeå¤§å°ï¼ŒBatch Sizeå¤ªå°æ—¶æ•ˆæœä¸å¥½

### 5.Why Batch Normalization Work? 

&emsp;&emsp;æŒ‰ç…§BNåŸæ–‡çš„è§£é‡Š(æœ¬æ–‡å‰3ç« )ï¼ŒBNå‡è½»äº†å±‚ä¹‹é—´è¾“å…¥çš„Internal Covariate Shiftã€‚ä½†ï¼š

* **[How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)**

&emsp;&emsp;**æ‘˜è¦**ï¼šæ–‡ç« é’ˆå¯¹BNåŸæ–‡çš„ICSå‡è±¡æå‡ºè´¨ç–‘ï¼Œå®éªŒè¡¨æ˜ï¼ŒBNå±‚å¹¶ä¸èƒ½å‡è½»ICSã€‚å®é™…ä¸ŠBNå¹³æ»‘äº†ç½‘ç»œçš„è§£ç©ºé—´è¿›è€Œå½±å“ä¼˜åŒ–æ•ˆç‡ã€‚


&emsp;&emsp;æ–‡ç« è®¾è®¡**å®éªŒä¸€**ï¼šè®¾è®¡VGGç½‘ç»œï¼Œåœ¨ä½¿ç”¨/ä¸ä½¿ç”¨BNå±‚ä¸‹åœ¨CIFAR10ä¸Šçš„æ•ˆæœã€‚

&emsp;&emsp;å®éªŒç›®çš„ï¼šéªŒè¯BNå±‚æ˜¯å¦çš„ç¡®æœ‰ç”¨ï¼Ÿ

<div align=center><img src="./figs/paper1_1.png"></div>

&emsp;&emsp;å®éªŒç»“æœï¼šåŠ å…¥BNå±‚çš„ç½‘ç»œè®­ç»ƒæ›´å¿«æ”¶æ•›ï¼Œèƒ½é€‚åº”æ›´å¤§çš„å­¦ä¹ ç‡ï¼Œæµ‹è¯•é›†è¯¯å·®ä½ï¼Œæ³›åŒ–æ•ˆæœå¥½ã€‚æ–‡ç« å°†æŸä¸€å±‚çš„ç½‘ç»œå‚æ•°åˆ†å¸ƒè¿›è¡Œå¯è§†åŒ–ï¼Œå‘ç°ä¸¤è€…åŒºåˆ«å¹¶ä¸æ˜æ˜¾ã€‚ä»è€Œä½œè€…æ€è€ƒï¼šBNå±‚çœŸçš„ä¼šå‡å°‘ICSï¼ŸICSåˆçœŸçš„ä¼šå½±å“è®­ç»ƒæ•ˆæœå—ï¼Ÿ


&emsp;&emsp;æ–‡ç« è®¾è®¡**å®éªŒäºŒ**ï¼šåœ¨BNå±‚ä¹‹åï¼Œåœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ï¼Œåœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ç»™æ•°æ®å¢åŠ ä¸€ä¸ªå™ªéŸ³ã€‚æ­¤ä¸¾ä¼šä¸¥é‡æŠ–åŠ¨æ•°æ®ï¼Œæ•°æ®åˆ†å¸ƒå˜å¾—æ›´åŠ æ‚ä¹±æ— ç« ï¼Œååˆ†ä¸ç¨³å®šã€‚åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸€ä¸ªä¸­é—´å±‚éƒ½æ¥å—ä¸€ä¸ª**ä¸åŒ**çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBN with Noise ä¸BNè¡¨ç°ç›¸å·®æ— å‡ ã€‚

&emsp;&emsp;å®éªŒç›®çš„ï¼šæ§åˆ¶è¾“å…¥æ•°æ®çš„å‡å€¼ä¸æ–¹å·®æ˜¯å¦ç›´æ¥å…³ç³»åˆ°è®­ç»ƒæ•ˆæœï¼ŸICSä¸è®­ç»ƒæ•ˆæœæ˜¯å¦æœ‰ç›´æ¥è”ç³»ï¼Ÿ

<div align=center><img src="./figs/paper1_2.png" width=75%></div>

&emsp;&emsp; å…¶ä¸­å™ªå£°ä»ä¸€ä¸ªå‡å€¼éé›¶ï¼Œæ ‡å‡†å·®éä¸€çš„åˆ†å¸ƒä¸­é‡‡æ ·å¾—æ¥ï¼Œè€Œè¯¥åˆ†å¸ƒçš„å‡å€¼ä¸æ–¹å·®ç”±å¦ä¸€ä¸ªåˆ†å¸ƒä¸­é‡‡æ ·è€Œæ¥ã€‚æ³¨æ„åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥tä¸­ï¼Œéƒ½è¦é‡å¤é‡‡æ ·ä¸€æ¬¡ã€‚
> &emsp;&emsp;noise sampled from a non-zero mean and non-unit variance distribution. We emphasize that this noise distribution changes at each time step.


&emsp;&emsp;å®éªŒç»“æœï¼šNoisy BN ä¸ Standard BN è®­ç»ƒæ•ˆæœæ— æ˜æ˜¾åŒºåˆ«ï¼ŒICSå¯¹è®­ç»ƒçš„å½±å“å¹¶æ²¡æœ‰æƒ³è±¡å¾—é‚£ä¹ˆå¤§ã€‚



&emsp;&emsp;æ–‡ç« è®¾è®¡**å®éªŒä¸‰**ï¼šè€ƒè™‘ä¸¤ä¸ªç½‘ç»œï¼ŒVGGä¸æ— æ¿€æ´»å‡½æ•°çš„çº¿æ€§æ·±åº¦ç½‘ç»œDLNã€‚å®šä¹‰ç¬¬$i$å±‚ä¸­é—´å±‚åœ¨ç¬¬$t$æ¬¡æ›´æ–°æ—¶çš„ICSä¸º$||G_{t,i}-G^{\prime}_{t,i}||_2$ï¼Œå…¶ä¸­ï¼š



$$
\begin{align}
&G_{t,i}=\nabla_{W_i^{t}}\mathcal{L}(W_1^{t},...,W_k^{t};x^{t},y^{t}) \\
&G_{t,i}^{\prime}=\nabla_{W_i^{(t)}}\mathcal{L}(W_1^{t+1},...,W_{i-1}^{t+1},W_i^t,..,W_k^t;x^{t},y^{t})
\end{align}
$$

> &emsp;&emsp;$G_{t,i}$ corresponds to the gradient of the layer parameters that would be applied during a simultaneous update of all layers (as is typical). On the other hand, $G^{\prime}_{t,i}$ is the same gradient after all the previous layers have been updated with their new values.


&emsp;&emsp;å®éªŒç›®çš„ï¼šBNçœŸçš„å‡å¼±äº†ICSï¼Ÿ

&emsp;&emsp;æ³¨æ„çš„æ˜¯DLNä¸­æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œç­‰ä»·ä¸€ä¸ªçº¿æ€§æ˜ å°„$Y=AX+b$ï¼Œâ€œç”±äºæ²¡æœ‰éçº¿æ€§æ¿€æ´»å±‚ï¼Œé‚£ä¹ˆæœ¬åº”å½“ä¸å‡ºç°ICSâ€ï¼ˆå¹¶ä¸æ˜¯å¾ˆç†è§£è¿™å¥è¯ï¼‰ã€‚

<div align=center><img src='./figs/paper1_3.png' width="80%"></div>'

&emsp;&emsp;å®éªŒç»“æœï¼šä»ä¸Šå›¾å·¦ä¸€åˆ—å¯ä»¥çœ‹å‡ºï¼ŒBNå±‚çš„ç¡®å¯¹è®­ç»ƒæœ‰æ‰€å¸®åŠ©ï¼Œå›¾å³ä¸¤åˆ—åˆ†åˆ«æ˜¯ä¸¤ä¸ªç½‘ç»œä¸­é—´å±‚çš„å‚æ•°ç©ºé—´ä¿¡æ¯ï¼šç¬¬ä¸€è¡Œè¡¨ç¤ºæ¢¯åº¦æ›´æ–°å‰åæ¢¯åº¦å¤§å°å˜åŒ–çš„$l_2$è·ç¦»(å³å‰é¢å®šä¹‰çš„ICS)(ç†æƒ³å€¼ä¸º0)ï¼Œç¬¬äºŒè¡Œè¡¨ç¤ºæ¢¯åº¦æ›´æ–°å‰åæ¢¯åº¦æ–¹å‘å˜åŒ–çš„è§’åº¦$cos$å€¼(ç†æƒ³ä¹‹ä¸º1ï¼Œå³è§’åº¦ä¸º0)ï¼›æœ´ç´ æƒ³æ³•å³å¦‚æœæ²¡æœ‰ä¸¥é‡ICSç°è±¡ï¼Œæ¢¯åº¦çš„æ¯ä¸€æ­¥åº”å½“å‘åŒæ ·çš„æ–¹å‘å‰è¿›ç›¸ç­‰çš„å¤§å°ï¼Œä½†å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹äºVGGæ¥è¯´ï¼ŒåŠ å…¥BNå±‚ICSå˜åŒ–ä¸æ˜æ˜¾ï¼Œå¯¹äºDLNæ¥è¯´ï¼ŒåŠ å…¥BNå±‚ICSåè€Œå˜å¤§ã€‚BNå±‚å¹¶ä¸èƒ½é™ä½ICSï¼Œåä¹‹ï¼Œä»–å¯èƒ½å¢åŠ ä¸­é—´å±‚ICSã€‚ï¼ˆéœ€è¦æ³¨æ„çš„æ˜¯â€œæ¢¯åº¦â€æŒ‡çš„æ˜¯Losså¯¹ä¸­é—´å±‚å‚æ•°çš„æ¢¯åº¦ã€‚ï¼‰

> &emsp;&emsp;This evidence suggests that, from optimization point of view, controlling the distributions layer inputs as done in BatchNorm, might not even reduce the internal covariate shift.

&emsp;&emsp;**ä»¥ä¸Šå®éªŒéƒ½è¡¨æ˜äº†ï¼ŒBNå±‚çš„ç¡®workï¼Œèƒ½å¤Ÿå¸®åŠ©è®­ç»ƒï¼Œä½†ICSä¸è®­ç»ƒæ•ˆæœæ— å…³ï¼ŒBNå±‚å¹¶ä¸èƒ½é™ä½ICSã€‚**é‚£ä¹ˆï¼ŒBNå±‚åˆ°åº•æ˜¯å¦‚ä½•ä¼˜åŒ–è®­ç»ƒçš„ï¼Ÿ



> &emsp;&emsp;Indeed, we identify the key impact that BatchNorm has on the training process: it reparametrizes the underlying optimization problem to make its landscape significantly more smooth.
> &emsp;&emsp;The loss changes at a smaller rate and the magnitudes of the gradients are smaller too 

&emsp;&emsp;ä»‹ç»ä¿©ä¸ªæ–°çš„æ¦‚å¿µï¼š

&emsp;&emsp;**1. åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°$\mathcal{L}$**

&emsp;&emsp;&emsp;&emsp;å¯¹äºå‡½æ•° $f$ è‹¥å­˜åœ¨å¸¸æ•° $L$ ä½¿å¾—å¯¹äº$\forall x_1,x_2 \in D$æœ‰$|f(x_1)-f(x_2)| \leq L|x_1 - x_2|$ï¼Œåˆ™ç§° $f$ ç¬¦åˆåˆ©æ™®å¸ŒèŒ¨æ¡ä»¶ï¼Œå¯¹äº $f$ æœ€å°çš„å¸¸æ•° $L$ ç§°ä¸º $f$ çš„**åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°**ã€‚

&emsp;&emsp;&emsp;&emsp;é€šä¿—æ¥è¯´ï¼ŒL-Lipschitzé™åˆ¶äº†å‡½æ•°çš„å˜åŒ–é€Ÿåº¦ï¼Œç¬¦åˆåˆ©æ™®å¸ŒèŒ¨æ¡ä»¶çš„å‡½æ•°æ–œç‡ä¸€å®šå°äºä¸€ä¸ªå®æ•°ï¼Œå³åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°ã€‚$x$ å˜åŒ–ä¸€å®šé‡ï¼Œå‡½æ•°ç›¸åº”å˜åŒ–çš„é‡ä¸èƒ½éå¸¸å¤§ã€‚å†ç®€å•ç‚¹è¯´å°±æ˜¯å‡½æ•°ä¸€é˜¶å¯¼æ•°è¦å°äºä¸€å®šå€¼ã€‚

&emsp;&emsp;**2. $\beta$-smoothness**

&emsp;&emsp;&emsp;&emsp;ç®€å•æ¥è¯´ï¼Œ$\beta$-smoothnesså°±æ˜¯å¯¹å‡½æ•°æ¢¯åº¦çš„ä¸€é˜¶å¯¼æ•°è¿›è¡Œé™åˆ¶ã€‚è§å…¬å¼ï¼š

$$
||\nabla f(x_1) - \nabla f(x_2)|| \leq \beta ||x_1 - x_2||
$$

&emsp;&emsp;åŸæ–‡è®¤ä¸ºBNä½¿å¾—loss landscapeæ›´åŠ å…‰æ»‘ã€‚è¯•æƒ³æ²¡æœ‰BNå±‚ä¹‹å‰çš„vanil DNNï¼ŒæŸå¤±å‡½æ•°ä¸ä»…éå‡¸ï¼Œä¸”å­˜åœ¨å¤§é‡â€œæ‰­ç»“â€ï¼Œå¹³å¦åŒºåŸŸï¼Œå°–é”çš„æå°å€¼ã€‚æ˜¾ç„¶è¿™ä¼šå¯¼è‡´éš¾ä»¥ä¼˜åŒ–ï¼Œä¼˜åŒ–ä¸ç¨³å®šã€‚

> &emsp;&emsp;the loss function is not only non-convex but also tends to have a large number of â€œkinksâ€, flat regions, and sharp minima.

&emsp;&emsp;BNå±‚ä½¿å¾—loss landscapeæ›´åŠ å¹³æ»‘ï¼ˆè§åŸæ–‡ğŸ‘‡ï¼‰

> &emsp;&emsp;After all, improved Lipschitzness of the gradients gives us confidence that when we take a larger step in a direction of a computed gradient, this gradient direction remains a fairly accurate estimate of the actual gradient direction after taking that step.

&emsp;&emsp;è¿™ä¹Ÿå°±æ„å‘³ç€æˆ‘ä»¬èƒ½å¤Ÿç”¨æ›´å¤§çš„å­¦ä¹ ç‡è€Œä¸å¿…æ‹…å¿ƒæœ€ä¼˜çš„lossæ–¹å‘çªç„¶å˜åŒ–å¯¼è‡´ä¸ç¨³å®šã€‚

> &emsp;&emsp;It thus enables any (gradientâ€“based) training algorithm to take larger steps without the danger of running into a sudden change of the loss landscape such as flat region (corresponding to vanishing gradient) or sharp local minimum (causing exploding gradients).

&emsp;&emsp;ä¸ºäº†éªŒè¯è¿™ä¸ªè¯´æ³•ï¼Œæ–‡ç« è®¾è®¡åœ¨with BN ä¸without BNçš„æƒ…å†µä¸‹å¯¹VGGç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚

<div align=center><img src='./figs/paper1_4.png' width="85%"></div>

<div align=center><img src='./figs/paper1_5.png', width='80%'></div>

&emsp;&emsp;fig(a)ï¼šåœ¨è®­ç»ƒä¸­çš„æ¯ä¸€ä¸ªstepï¼Œè®¡ç®—å½“å‰stepæŸå¤±å‡½æ•°çš„æ–¹å‘ï¼Œæ²¿ç€è¿™ä¸ªæ–¹å‘èµ°ä¸‹å»çš„losså˜åŒ–èŒƒå›´ï¼›æ³¨æ„å›¾ä¸­ä¸æ˜¯æ›²çº¿ã€‚

> &emsp;&emsp;To demonstrate the impact of BatchNorm on the stability of the loss itself, i.e., its Lipschitzness, for each given step in the training process, we compute the gradient of the loss at that step and measure how the loss changes as we move in that direction â€“ see Figure 4(a)

&emsp;&emsp;fig(b)ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æŸç‚¹çš„æŸå¤±å‡½æ•°æ¢¯åº¦ï¼Œä¸ä¸Šä¸€ä¸ªæ¢¯åº¦æ–¹å‘ä¸åŒçš„ç‚¹çš„L2è·ç¦»å˜åŒ–ã€‚

> &emsp;&emsp;Similarly, to illustrate the increase in the stability and predictiveness of the gradients, we make analogous measurements for the $l_2$ distance between the loss gradient at a given point of the training and the gradients corresponding to different points along the original gradient direction.

&emsp;&emsp;fig(c)ï¼šæ²¿ç€æ¢¯åº¦æ–¹å‘ä¸Šï¼Œæ¢¯åº¦çš„ $\beta$ å¸¸æ•°èŒƒå›´ã€‚

> &emsp;&emsp;To further demonstrate the effect of BatchNorm on the stability/Lipschitzness of the gradients of the loss, we plot in Figure 4(c) the â€œeffectiveâ€ Î²-smoothness of the vanilla and BatchNorm networks throughout the training. (â€œEffectiveâ€ refers here to measuring the change of gradients as we move in the direction of the gradients.).

&emsp;&emsp;é’ˆå¯¹ä¹‹å‰çš„å®éªŒä¸‰ï¼Œæœ‰æ²¡æœ‰BNå±‚å¯¹VGGæ¥è¯´ï¼ŒICSæŒ‡æ ‡å˜åŒ–ä¸å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸¤è€…çš„å‚æ•°æ›´æ–°å‰åæ¢¯åº¦L2å˜åŒ–ä¸å¤§ï¼Œè§’åº¦å˜åŒ–çš„åŒºåˆ«ä¸å¤§ã€‚withoutBNçš„æŸå¤±å‡½æ•°é™¡å³­ä¸è§„åˆ™ï¼ˆä¸‹å›¾å·¦ï¼‰ï¼Œé€ æˆä»–çš„æ¢¯åº¦è¦æ¥å›å˜åŒ–ï¼Œé‚£æŒ‰ç…§è¿™é‡Œçš„è§£é‡Šï¼ŒwithBNçš„æŸå¤±å‡½æ•°å¹³æ»‘è®¸å¤šï¼ˆä¸‹å›¾å³ï¼‰ï¼Œé‚£ä¸ºä»€ä¹ˆä»–çš„æ¢¯åº¦å¤§å°è¿˜è¦æ¥å›å˜åŒ–ï¼Œæ–¹å‘æ¥å›å˜åŒ–ï¼Ÿæ¢¯åº¦æ–¹å‘ä¸åº”è¯¥å˜åŒ–æ¯”è¾ƒå°å—ï¼Ÿ

&emsp;&emsp;çŒœæµ‹å¯èƒ½æ˜¯å®ƒæ˜¯â€œèºæ—‹â€ä¸‹é™ï¼Œè™½ç„¶â€œç»å¯¹â€æ–¹å‘åœ¨å˜ï¼Œä½†ç›¸å¯¹æ¥è¯´éƒ½æ˜¯å‘ä¸‹çš„ï¼Œä¸åƒå·¦å›¾ä¸€æ ·ä¼šè·³å‡ºè¯¥ç‚¹è·‘åˆ°åˆ«çš„åœ°æ–¹ï¼Ÿ

<div align=center><img src='./figs/paper1_6.png', width='60%'></div>

&emsp;&emsp;é‚£æ˜¯å¦åªæœ‰Batch Normalizationä¸€ç§æ–¹æ³•ä¼˜åŒ–æŸå¤±å‡½æ•°ç©ºé—´/æˆ–æ˜¯Batch Normalizationå°±æ˜¯æœ€å¥½çš„ï¼Ÿæ–‡ç« ç”¨ $L_p$ æ­£åˆ™åŒ–ä»£æ›¿BNä¹Ÿè¾¾åˆ°äº†ç±»ä¼¼çš„æ•ˆæœã€‚

&emsp;&emsp;**ç†è®ºåˆ†æ**ï¼ŒåŸæ–‡è€ƒè™‘ä¸€ä¸ªVanilla Networkä¸Vanilla Network + BatchNorm Layerçš„æŸå¤±å‡½æ•°åŒºåˆ«ã€‚æœ€ç»ˆå¾—å‡ºï¼ŒåŠ å…¥BNå±‚ä¹‹åï¼ŒæŸå¤±å‡½æ•°æ¢¯åº¦æœ‰ä¸€ä¸ªç›¸åº”çš„ä¸Šç•Œï¼ˆL-Lipschitznessï¼‰å³æŸå¤±å‡½æ•°æ›´åŠ åˆ©æ™®å¸Œå…¹ï¼Œå¼•å…¥äº† BN åï¼ŒæŸå¤±å‡½æ•°ç›¸å¯¹äºæ¿€æ´»å‡½æ•°å€¼çš„äºŒé˜¶é¡¹å¹…å€¼æ›´å°ï¼Œä¹Ÿå³æŸå¤±å‡½æ•°æ›´åŠ è´å¡”å¹³æ»‘ã€‚

* **[Understanding Batch Normalization(NIPS-2018)](https://arxiv.org/abs/1806.02375)**

&emsp;&emsp;**æ‘˜è¦**:XXXXXXXX

&emsp;&emsp;**ç¬¬ä¸€èŠ‚**è®¾è®¡å®éªŒéªŒè¯BNå±‚åˆ°åº•æœ‰æ²¡æœ‰ç”¨ï¼ŸResnet110+SGD

<div align=center><img src='./figs/paper2_1.png'></div>

&emsp;&emsp;å®éªŒç»“æœä¹Ÿè¡¨æ˜äº†BNå±‚çš„ç¡®å¯¹è®­ç»ƒæœ‰æ‰€å¸®åŠ©ï¼Œæ›´é«˜çš„å­¦ä¹ ç‡ï¼Œæ›´å¿«æ”¶æ•›ï¼Œæ³›åŒ–èƒ½åŠ›å¥½ã€‚

&emsp;&emsp;**ç¬¬äºŒèŠ‚**é’ˆå¯¹SGDæ¨¡å‹è€ƒè™‘ç»éªŒåŒ–çš„ç†è®ºåˆ†æï¼š

&emsp;&emsp;æŸå¤±å‡½æ•°æœ‰$\ell (x)=\frac{1}{N} \sum_{i=1}^{N}\ell _i(x)$ï¼ŒæŒ‰ç…§å½“ä¸‹batch-SGDç®—æ³•ï¼Œè€ƒè™‘batchä¸‹é›†åˆ$B$ä¸ºæ•°æ®é›†ä¸€ä¸ªå­é›†ï¼Œå­¦ä¹ ç‡$\alpha$ï¼Œåˆ™æ¯ä¸€æ­¥æ¢¯åº¦æ›´æ–°$\alpha \nabla_{SGD}(x) = \frac{\alpha}{|B|} \sum_{i \in B} \nabla \ell _i(x)$ã€‚å¯¹è¯¥å¼åšç®€å•åŠ å‡æœ‰ï¼š

$$
\alpha \nabla_{SGD}(x) = \underbrace{ \alpha \nabla \ell (x) }_{gradient} + \underbrace{\frac{\alpha}{|B|}\sum_{i \in B}(\nabla \ell _i(x) - \nabla \ell (x))}_{error\ term}
$$

&emsp;&emsp;è€ƒè™‘åˆ°å‰éƒ¨åˆ†ä¸ºåŸæœ¬æ¢¯åº¦æ›´æ–°å¤§å°ï¼ŒååŠéƒ¨åˆ†ä¸ºè¯¯å·®é¡¹ï¼Œç”±äºæˆ‘ä»¬åªåœ¨ä¸€ä¸ªbatchç§åšæ±‚å’Œå¹³å‡æ“ä½œï¼Œæ‰€ä»¥ä¸€å®šç¨‹åº¦ä¸Šï¼Œæ¢¯åº¦çš„æ›´æ–°ä¸true gradientæœ‰ä¸€å®šè¯¯å·®ï¼Œç›¸å½“äºå¼•å…¥äº†å™ªéŸ³ã€‚ç”±äºæˆ‘ä»¬batchå¯¹æ•°æ®é›†å‡åŒ€é‡‡æ ·ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹æ¢¯åº¦çš„ä¼°è®¡ä¸ºæ— åä¼°è®¡ï¼Œå³ï¼š

$$
\mathbb{E}[\frac{\alpha}{|B|}\sum_{i \in B}(\nabla \ell _i(x) - \nabla \ell (x))]=0
$$

&emsp;&emsp;å–å™ªå£°éƒ¨åˆ†ä¸º$C=\mathbb{E}[||\nabla \ell _i (x) - \nabla \ell  (x)||^2]$ï¼Œé™„å½•Dï¼Œå¯æ¨å‡ºä¸Šç•Œï¼š
$$
\mathbb{E}[||\alpha \nabla \ell (x) - \alpha \nabla_{SGD}(x)||^2] \leq \frac{\alpha ^2}{|B|}C
$$

&emsp;&emsp;ä»è¿™ä¸ªä¸Šç•Œå¯ä»¥çœ‹å‡ºï¼Œå­¦ä¹ ç‡è¶Šå¤§ï¼Œbatch sizeè¶Šå°ï¼ŒSGDæ¢¯åº¦ä¸Šç•Œå°±è¶Šå¤§ï¼Œæ¢¯åº¦æ›´æœ‰å¯èƒ½è·³åˆ°â€œæ›´è¿œâ€çš„åœ°æ–¹ï¼Œä¹Ÿå°±ç›¸åº”æ³›åŒ–èƒ½åŠ›æ›´å¥½ã€‚

> &emsp;&emsp; it is empirically demonstrated that large mini-batches lead to convergence in sharp minima, which often generalize poorly. 
> &emsp;&emsp;The intuition is that larger SGD noise from smaller mini-batches prevents the network from getting â€œtrappedâ€ in sharp minima and therefore bias it towards wider minima with better generalization. Our observation implies that SGD noise is similarly affected by the learning rate as by the inverse mini-bath size, suggesting that a higher learning rate would similarly bias the network towards wider minima.

&emsp;&emsp;é‚£ä¹ˆBatch Normalizationåˆæ˜¯å¦‚ä½•å½±å“å­¦ä¹ ç‡ï¼Œä½¿å¾—é«˜çš„å­¦ä¹ ç‡ä¸ä¼šå‘æ•£ï¼Ÿ

&emsp;&emsp;ç”±äºåœ¨å¤§çš„å­¦ä¹ ç‡ä¸‹ï¼Œwithout BNç½‘ç»œåœ¨æœ€åˆå‡ ä¸ªstepå¾ˆå®¹æ˜“å‘æ•£ï¼Œæ‰€ä»¥æ–‡ç« é€‰æ‹©æœ€åˆçš„æ¢¯åº¦å¹…åº¦åˆ†å¸ƒã€‚å…·ä½“è€Œè¨€ï¼ŒåŸæ–‡é€‰æ‹©æœ€å¼€å§‹çš„ç¬¬55å±‚å·ç§¯æ ¸çš„æ¢¯åº¦å¹…å€¼åˆ†å¸ƒã€‚

<div align=center><img src='./figs/paper2_2.png'></div>


&emsp;&emsp;é™¤æ­¤ä¹‹å¤–ï¼Œè€ƒè™‘æ²¿ç€æ¢¯åº¦æ–¹å‘çš„relative loss(i.e. new_loss/old_loss)éšç€step-sizeå˜åŒ–çš„æ›²çº¿å¦‚ä¸‹ã€‚

> &emsp;&emsp;A natural way of investigating divergence is to look at the loss landscape along the gradient direction during the first few mini-batches that occur with the normal learning rate (0.1 with BN, 0.0001without).

<div align=center><img src='./figs/paper2_3.png'></div>

&emsp;&emsp;ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œéšç€step-sizeå¢å¤§ï¼Œ with BNçš„relative lossåœ¨å¾ˆå¤§èŒƒå›´å†…éƒ½æ§åˆ¶åœ¨1ä»¥ä¸‹æˆ–é™„è¿‘ï¼Œè€Œw/o BNå¾ˆå®¹æ˜“è¿œè¿œæ¯”1å¤§ï¼Œå¾ˆå®¹æ˜“é«˜å‡ºä¸¤ä¸ªé‡çº§ã€‚ï¼ˆå¦‚æœrelative loss å¾ˆå¤§ï¼Œè¯´æ˜ç½‘ç»œâ€œè¿ˆå‡ºâ€è¿™ä¸€æ­¥ä¹‹ålossæ˜æ˜¾æ¯”ä¸Šä¸€æ­¥è¦å¤§ï¼Œè¯´æ˜å¾ˆå®¹æ˜“å‘æ•£ã€‚ï¼‰


&emsp;&emsp;åŸæ–‡è€ƒè™‘without BNç½‘ç»œä¸­é—´å±‚æ¿€æ´»è¾“å‡ºçš„å‡å€¼ä¸æ–¹å·®å˜åŒ–ã€‚å…¶ä¸­é¢œè‰²æ¡æŒ‡åä¸€å±‚çš„è¾“å‡ºä¸å‰ä¸€å±‚è¾“å‡ºçš„æ¯”å€¼ã€‚ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œéšç€stepè¿›è¡Œï¼Œæ¨¡å‹å‘æ•£ï¼Œå‡å€¼é€æ¸åç§»ï¼Œæ–¹å·®é€æ¸å¢å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´è¾“å‡ºâ€œçˆ†ç‚¸â€ã€‚å±‚æ•°è¶Šæ·±çš„è¾“å‡ºï¼Œâ€œçˆ†ç‚¸â€ç°è±¡æ˜æ˜¾ã€‚

<div align=center><img src='./figs/paper2_4.png'></div>

> &emsp;&emsp;The color bar reveals that the scale of the later layerâ€™s activations and variances is orders of magnitudes higher than the earlier layer. This seems to suggest that the divergence is caused by activations growing progressively larger with network depth, with the network output â€œexplodingâ€ which results in a diverging loss. BN successfully mitigates this phenomenon by correcting the activations of each channel and each layer to zero-mean and unit standard deviation, which ensures that large activations in lower levels cannot propagate uncontrollably upwards.

&emsp;&emsp;å¦ä¸€ä¸ªå®éªŒæ•°æ®ï¼š

<div align=center><img src='./figs/paper2_5.png'></div>

&emsp;&emsp;è‡³æ­¤æ‰€åšçš„å®éªŒï¼Œæˆ‘ä»¬å¯å¾—å‡ºç»“è®ºï¼š1ï¼Œå¤§çš„å­¦ä¹ ç‡ï¼Œæ¨¡å‹â€œè¿ˆâ€å¾—è¶Šè¿œï¼Œæ›´å®¹æ˜“è·³å‡ºsharp minimaã€‚ï¼ˆä½†loss landscapeå¦‚æœä¸å¹³å¦ï¼Œâ€œè¿ˆâ€å¾—è¶Šè¿œï¼Œè¶Šä¸å®¹æ˜“æ”¶æ•›ï¼‰2ï¼Œw/o BNç›¸æ¯”äºwith BNçš„æ¢¯åº¦æ›´æ–°æ­¥é•¿èŒƒå›´å¤ªå¹¿ã€‚ï¼ˆFigure 2.ï¼‰3ï¼ŒBNå±‚ä¼šä½¿å¾—loss landscapeæ›´åŠ å¹³å¦ï¼Œåœ¨åŒæ ·è¿œçš„èŒƒå›´å†…ï¼Œwith BNçš„æ¢¯åº¦å˜åŒ–ä¸å¤§ã€‚4ï¼Œw/o BNä¼šå¯¼è‡´æ•°æ®åˆ†å¸ƒé€æ¸å˜åŒ–ï¼Œå‡å€¼åç§»ï¼Œæ–¹å·®å˜å¤§ã€‚ï¼ˆä¸çŸ¥é“è¿™æ ·ç®—ä¸ç®—ICSçš„ä¸€ç§è®ºæ®ï¼Œç»“åˆä¸Šä¸€ç¯‡ï¼Œw/o BN æ•°æ®ICSï¼Œä½†ICSä¸ä¼šå½±å“è®­ç»ƒï¼ŒBNå±‚åè€Œå¢å¤§ICSã€‚ï¼‰


* **[An Empirical Analysis of theOptimization of Deep Network Loss Surfaces](https://arxiv.org/abs/1612.04010)**



å¾…ç»­ + BN backward

### 6.Q&A

* ä¸ºä»€ä¹ˆéœ€è¦$\beta$ä¸$\gamma$ï¼Œå³ä¸ºä»€ä¹ˆéœ€è¦scale and shiftè¿‡ç¨‹ï¼Ÿ

&emsp;&emsp;BatchNormæœ‰ä¸¤ä¸ªè¿‡ç¨‹ï¼ŒStandardizationå’Œscale and shiftï¼Œå‰è€…å°†mini batchæ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œè€Œåè€…åˆ™è´Ÿè´£æ¢å¤æ•°æ®æœ¬èº«æºå¸¦çš„ä¿¡æ¯ï¼Œè¯•æƒ³æ²¡æœ‰æœ€åçš„scale and shiftè¿‡ç¨‹ï¼Œæ‰€æœ‰batchçš„è¾“å…¥æ•°æ®éƒ½ä¼šè¢«æ ‡å‡†åŒ–ï¼Œæ ‡å‡†åŒ–æœ¬èº«æœ‰åˆ©äºæ›´æ–°æƒé‡ï¼Œå› ä¸ºæ‰€æœ‰è¾“å…¥çš„æ•°æ®åˆ†å¸ƒè¿‘ä¹ä¸€è‡´ï¼Œä¸æ ‡å‡†åŒ–æœ‰åˆ©äºä¿æŠ¤æ•°æ®æœ¬èº«åˆ†å¸ƒæ‰€æºå¸¦çš„ä¿¡æ¯ã€‚**è€Œscale and shiftå°±æ˜¯åœ¨åˆ†å¸ƒä¸æƒé‡ä¹‹é—´å®ç°å¹³è¡¡**ï¼Œè€ƒè™‘$\gamma$=1,$\beta$=0ç­‰ä»·äºåªç”¨Standardizationï¼Œä»¤$\gamma$=$\sigma$,$\beta$=$\mu$ç­‰ä»·äºæ²¡æœ‰BNå±‚ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®©losså†³å®šä»€ä¹ˆæ ·çš„åˆ†å¸ƒæ˜¯åˆé€‚çš„ã€‚

* BNå±‚æ”¾åœ¨ReLUå‰é¢è¿˜æ˜¯åé¢ï¼Ÿ

&emsp;&emsp;Sigmoidæ¿€æ´»å‡½æ•°å…·æœ‰é¥±å’Œæ€§å¯èƒ½é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œé‚£å¯¹äºå…·æœ‰å³é¥±å’Œæ€§çš„ReLUæ¿€æ´»å‡½æ•°å‘¢æ¥è¯´ï¼ŒBNå±‚æ”¾åœ¨å…¶å‰é¢è¿˜æ˜¯åé¢ã€‚Batch NoralizationåŸæ–‡å»ºè®®å°†BNå±‚æ”¾åœ¨ReLUä¹‹å‰ï¼Œå› ä¸ºReLUæ¿€æ´»å‡½æ•°çš„è¾“å‡ºéè´Ÿï¼Œä¸èƒ½è¿‘ä¼¼ä¸ºé«˜æ–¯åˆ†å¸ƒã€‚
> &emsp;&emsp;The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments **we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution.**

&emsp;&emsp;ä½†åœ¨ [caffenet-benchmark](https://github.com/ducha-aiki/caffenet-benchmark#batch-normalization)ä¸­ï¼Œä½œè€…åŸºäºcaffenetåœ¨ImageNetä¸Šåšäº†å¯¹æ¯”å®éªŒï¼Œå®éªŒè¡¨æ˜ï¼Œæ”¾åœ¨å‰åçš„å·®å¼‚ä¼¼ä¹ä¸å¤§ï¼Œç”šè‡³æ”¾åœ¨ReLUåè¿˜å¥½ä¸€äº›ã€‚
|Name|Accuracy|LogLoss|Comments|
|:------|:-----:|:-----:|:-----:|
|Before|0.474|2.35|As in Paper|
|Before+scale&bias layer|0.478|2.33|As in Paper|
|After|**0.499**|**2.21**||
|After+scale&bias layer|0.493|2.24||

---
### Ref

* [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift(arXiv)](https://arxiv.org/abs/1502.03167) 

* [How Does Batch Normalization Help Optimization?(NIPS-2018)](https://arxiv.org/abs/1805.11604) 

* [Understanding Batch Normalization(NIPS-2018)](https://arxiv.org/abs/1806.02375)

* [An Empirical Analysis of theOptimization of Deep Network Loss Surfaces](https://arxiv.org/abs/1612.04010)

* [The Gradient Flow through the Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)

* [ã€ŠHow Does Batch Normalization Help Optimizationã€‹ç¬”è®°â€”â€”CapsulE](https://zhuanlan.zhihu.com/p/72912402)

* [è®ºæ–‡|How Does Batch Normalizetion Help Optimizationâ€”â€”Estyle](https://zhuanlan.zhihu.com/p/66683061)

* [How Does Batch Normalization Help Optimization?â€”â€”cnblogs](https://www.cnblogs.com/seniusen/p/10795297.html)