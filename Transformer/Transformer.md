# Transformer
---
（Attention is all you need）
Transformer在做什么？Transformer本身即Encoders-Decoders结构，（Seq2Seq结构）6个Encoder结构完全相同，6个Decoder结构完全相同，但Encoder与Decoder结构不同，6个Encoder/Dencoder的结构完全相同，参数不相同。
1、pos embedding
   Word Embedding + Pos Embedding(位置信息)
2、Attention机制
 

### Ref
---
[Transformer](https://www.bilibili.com/video/BV1Di4y1c7Zm?from=search&seid=3862635135511267341)